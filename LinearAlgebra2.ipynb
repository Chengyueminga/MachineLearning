{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Albegra Review and Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Concepts and Notation\n",
    "## 2. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vector-Vector Products\n",
    "#### Given two vectors $x, y \\in R^n$, the quantity $x^Ty$, sometimes called the *inner product* or *dot product*.  \n",
    "#### $x^Ty \\in R = [x_1,x_2,......,x_n][y_1,y_2,......y_n]^T = \\sum_{i=1}^{n}{x_iy_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **outer product**\n",
    "#### Given vectors $x\\in R^m, y \\in R^n$(not necessarily of the same size), $xy^T \\in R^{m*n}$ is called the **outer product** of the vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left[\n",
    "\\begin{matrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_n\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "        [y_1,y_2,...,y_n] = \\left[\\begin{matrix}x_1*y_1&x_1*y_2&...&x_1*y_n\\\\\n",
    "                                   x_2*y_1&x_2*y_2&...&x_2*y_n\\\\\n",
    "                                   .......\\\\\n",
    "                                   x_m*y_1&x_m*y_2&...&x_m*y_n\n",
    "                                   \\end{matrix}\n",
    "                                   \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matrix-Vector Products\n",
    "$$ y = xA = \\left[\n",
    "              \\begin{matrix}\n",
    "              x_1,x_2 ......x_n\n",
    "              \\end{matrix}\n",
    "              \\right]\n",
    "        \\left[\n",
    "        \\begin{matrix}\n",
    "        ---a^T_1---\\\\\n",
    "        ---a^T_2---\\\\\n",
    "        .......\\\\\n",
    "        ---a^T_m---\n",
    "        \\end{matrix}\n",
    "        \\right] = x_1[---a^T_1---] + x_2[---a^T_2---]+.....+x_n[---a^T_n---]\n",
    "              $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Matrix-Matrix Products\n",
    "### C = X*Y\n",
    "$$C = XY = \\left[\\begin{matrix}x_1^T*y_1&x_1^T*y_2&...&x_1^T*y_n\\\\\n",
    "                                   x_2^T*y_1&x_2^T*y_2&...&x_2^T*y_n\\\\\n",
    "                                   .......\\\\\n",
    "                                   x_m^T*y_1&x_m^T*y_2&...&x_m^T*y_n\n",
    "                                   \\end{matrix}\n",
    "                                   \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The direct advantage of these various viewpoints is that they allow you to operate on the level/unit of vectors instead of scalars.\n",
    "#### Matrix multiplication is associative: (AB)C = A(BC)\n",
    "#### Matrix multiplication is distributive: A(B + C) = A*B + A*C\n",
    "#### Matrix multiplication is, in general, *not* commutative; that is, it can be the case that $AB \\not= BA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Operations and Properties\n",
    "### 3.1 The Identity Matrix and Diagonal Matrics\n",
    "#### *identity matrix*\n",
    "$I \\in R^{n*n}$ \n",
    "$$I_{ij} = 1, i= j$$\n",
    "$$I_{ij} = 0, i \\not= j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *diagonal matrix*\n",
    "#### D = diag{$d_1,d_2,...d_n$}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Transpose\n",
    "$A^T_{ij} = A{ji}$\n",
    "#### The following properties of transposes are easily verfied:\n",
    "### $(A^T)^T = A$   \n",
    "### $(AB)^T = B^TA^T$  \n",
    "### $(A+B)^T = A^T + B^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Symmetric Matrics\n",
    "#### *symmetric* if $A = A ^T$  \n",
    "#### *anti-symmetric* if $A = - A^T$  \n",
    "#### It is easy to show that for any matrix $A \\in R ^{n*n}$, the matrix$A + A^T$ is symmetric and the matrix$A - A^T$ is anti-symmetric\n",
    "#### $A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The Trace\n",
    "### $$trA = \\sum_{i = 1}^{n}A_{ii}$$\n",
    "#### For $A \\in R{n*n}, trA = trA^T$\n",
    "#### For $A,B \\in R^{n * n}, tr(A + B) = trA + trB$\n",
    "#### For $A \\in R^{n * n}, t \\in R, tr(tA) = t*trA$\n",
    "#### For$ A, B$ such that $AB$ is square, $trAB = trBA$  \n",
    "#### For $A,B,C$ such that $ABC$ is square, $trAB = trBA$\n",
    "#### For $A,B,C$ such that $AB$ is square,$trAB = trBA$\n",
    "#### For $A,B,C$ such that $ABC$ is square, $trABC = trBCA = trCAB$, and so on for the product of more matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Norms\n",
    "#### A *norm* of a vector ||x|| is informally a measure of the \"length\" of the vector. For example, we have the commoly-used Euclidean or $\\cal l_2$ norm,\n",
    "$$||x||_2 = \\sqrt{\\sum^n_{i = 1}x^2_i}$$\n",
    "#### Note that $||x||^2_2 = x^Tx$  \n",
    "#### 1.For all $x \\in R^n,f(x) \\geq 0$ (non-negativity).\n",
    "#### 2.$f(x) = 0$if and only if $x = 0$ (definiteness).  \n",
    "#### 3.For all $x \\in R^n, t \\in R, f(tx) = |t|f(x)$ (homogeneity).\n",
    "#### 4.For all$x,y \\in R^n, f(x + y) \\leq f(x) + f(y)$ (triangle inequality).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Linear Independence and Rank\n",
    "#### *(linearly) independent* \n",
    "#### A set of vectors${x_1,x_2,......x_n} \\subset R^m$ is said to be *(linearly) independent* if no vector can be represented as a linear combination of the remaining vectors.\n",
    "#### *(linearly) dependent*  \n",
    "#### Converselt, if one vector belonging to the set can be represented as a linear combination of the remaining vectors, then the vectirs are said to be *(linearly) dependent*, That is, if  \n",
    "$$x_n = \\sum^{n - 1}_{i = 1}\\alpha_ix_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Example\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.matrix(\"1;2;3\")\n",
    "x2 = np.matrix(\"4;1;5\")\n",
    "x3 = np.matrix(\"2;-3;-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 == -2 * x1 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For$A \\in R^{m*n}, rand(A) \\leq min(m,n). $If$ rank(A) = min(m,n),$ then $A$ is said to be *full rank*\n",
    "#### For$A \\in R^{m*n}, rank(A) = rank(A^T)$\n",
    "#### For $A \\in R^{m*n}, B \\in R^{n*p}, rank(AB) \\leq min(rank(A),rank(B))$\n",
    "#### For $A,B \\in R^{m*n}, rank(A + B) \\leq rank(A) + rank(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 The Inverse of a Square Matrix\n",
    "#### The *inverse* of a square matrix $A \\in R^{n * n}$ is denoted $A^{-1}$, and is the unique matrix such that  \n",
    "$$A^{-1}A = I = AA^{-1}$$\n",
    "#### The following are properties of the inverse; all assumes that $A,B \\in R^{n*n}$ are non-singular (invertible):  \n",
    "#### $(A^{-1})^{-1} = A$\n",
    "#### $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "#### $(A^{-1})^{T} = (A^{T})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Orthogonal Matrices\n",
    "#### Two vectors $x,y \\in R^n$are *orthogonal* if $x^Ty = 0$.   \n",
    "#### A vector $x \\in R^n$ is *normalized* if $||x||_2$ = 1  \n",
    "#### A square matrix $U \\in R^{n*n}$ is *orthogonal* if all its columns are orthogonal to each other and are normalized.  ### It follows immediately from the definition of orthogonality and normality that  \n",
    "$$U^TU = I = UU^T$$  \n",
    "#### Another nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix will not change its Euclidean norm, i.e.,\n",
    "$$||Ux||_2 = ||x||_2$$\n",
    "#### for any $x \\in R^n, U \\in R^{n*n}$ orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Range and Nullspace of a Matrix\n",
    "#### The *span* of a set of vectors $\\{x_1,x_2,.....x_n\\}$ is the set of all vectors that can be expressed as a linear combination of ${x_1,x_2,......x_n}$, That is,  \n",
    "$$span({x_1,...x_n}) = \\{ v:v = \\sum^{n}_{i = 1} \\alpha_ix_i, \\alpha_i \\in R \\}$$\n",
    "#### It can be shown that if $\\{x_1,x_2,...x_n\\}$ is a set of *n* linearly independent vectors, where each $x_i \\in R^n$, then $span(\\{x_1,...x_n\\}) = R^n$  \n",
    "#### In other words, *any* vector $v \\in R^n$ can be written as a linear combination of $x_1$ through $x_n$  \n",
    "#### The *projection* of a vactor $y \\in R^m$ onto the span of $\\{x_1,x_2,...x_n\\}$, such that $v$ is as close as possible to *y*, as measured by the Euclidean norm $||v - y||_2$. We denote the projection as Proj$(y;\\{x_1,x_2,...x_n\\}$ and can define it formally as,  \n",
    "$$Proj(y;\\{x_1,...x_n\\} = argmin_{v\\in span(\\{x_1,...x_n\\})}||y - v||_2$$\n",
    "#### The *range* (sometimes also called the columnspace) of a matrix $A \\in R^{m*n}$, denoted $\\mathcal{R}(A)$, is the span of the columns of *A*. In other words,  \n",
    "$$\\mathcal{R}(A) = \\{v \\in \\mathfrak{R}^m : v = Ax, x \\in \\mathfrak{R}^n\\}$$\n",
    "#### Making a few technical assumptions (namely that A id full rank and that n < m), the projection of a vector $y \\in \\mathfrak{R}^n$ onto the range of *A* is given by,  \n",
    "$$Proj(y;A) = argmin_{v \\in \\mathcal{R}(A)||v - y||_2} = A(A^TA)^{-1}A^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The *nullspace* of a matrix $A \\in R^{m*n}$, denoted$\\mathcal{N}(A)$ is the set of all vectors that equal 0 when multiplied by *A*, i.e.,  $$\\mathcal{N}(A) = \\{x \\in R^n: Ax = 0\\}$$\n",
    "#### Note that vectors in $\\mathcal{N}(A) \\in R^{m*n}$, denoted $\\mathcal{N}(A)$ are of size *n*, so vectors in $\\mathcal{R}(A^T)$ and $\\mathcal{N}(A)$ are both in $R^n$. In fact, we can say much more. I t turns out that  \n",
    "$$\\{w: w = u + v, u \\in \\mathcal{R}(A^T), v \\in \\mathcal{N}(A)\\} = R^n and (\\mathcal{R}(A^T)\\bigcap\\mathcal{N}(A) = \\{0\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 The Determinant\n",
    "#### The *determinant* of a square matrix $A \\in R^{n*n}$, is a function det: $R^{n*n}\\rightarrow R$,is denoted $|A|$ or det$A$.\n",
    "$$S = \\{v \\in R^n: v = \\sum_{i = 1}^{n}\\alpha_ia_i (where 0 \\leq \\alpha_i \\leq 1,i =1,....n)\\}$$\n",
    "#### Several properties that follow from the three properties above include:  \n",
    "#### For $A \\in R^{n*n}, |A| = |A^T|$\n",
    "#### For $A,B \\in R^{n*n}, |AB| = |A||B|$\n",
    "#### For$A \\in R^{n*n}, |A| = 0$ if and only *A* is singular (i.e., non-invertible). (If *A* is singular then it does not have full rank, and hence its columns are linearly dependent. In this case, the set *S* corresponds to a \"flat sheet\" within the *n*-dimensional space and hence has zero volume).  \n",
    "#### For $A \\in R^{n*n}$ and *A* non-singular, $|A^{-1}| = 1/|A|$  \n",
    "#### The *classical adjoint* (often just called the adjoint) of a matrix $A \\in R^{n*n}$, is denoted adj(A), and defined as\n",
    "$$A^{-1} = \\frac{1}{|A|}adj(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Quadratic Forms and Positive Semidefinite Matrices\n",
    "#### Given a square matrix $A \\in R^{n*n}$ and a vector $x \\in R^n$, the scalar value $x^TAx$ is called a *quadratic form*.  \n",
    "$$x^TAx = \\sum_{i = 1}^{n}\\sum_{j = 1}^{n}A_{ij}x_iy_j$$\n",
    "#### Note that,  \n",
    "$$x^TAx = (x^TAx)^T = x^TA^Tx = x^T(\\frac{1}{2}A + \\frac{1}{2}A^T)x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 Eigenvalues and Eigenvectors  \n",
    "#### Given a square matrix $A \\in R^{n*n}$, we say that $\\lambda \\in C$ is an *eigenvalue* of *A* and $x \\in C^n$ is the corresponding *eigenvector* if  \n",
    "$$Ax = \\lambda x, x \\neq 0$$\n",
    "#### We can rewrite the equation above to state that $(\\lambda,x)$ is an eigenvalue-eigenvector pair of *A* if,  \n",
    "$$(\\lambda I - A)x = 0, x \\neq 0$$\n",
    "#### But $$(\\lambda I - A)x = 0$$ has a non-zero solution to $x$ if and only if $(\\lambda I - A)$ has a non-empty nullspace, which is only the case if $|(\\lambda I  - A)| = 0$\n",
    "#### The following are properties of eigenvalues and eigenvectors (in all cases assume $A \\in R^{n*n}$ has eigenvalues $\\lambda_i,.....\\lambda_n$):\n",
    "#### The trace of a *A* is equal to the sum of its eigenvalues,  \n",
    "$$trA = \\sum_{i = 1}^{n}\\lambda_i$$\n",
    "#### The determinant of *A* is equal to the product of its eigenvalues,\n",
    "$$|A| = \\prod_{i = 1}^{n}\\lambda_i$$\n",
    "#### The rank of *A* is equal to the number of non-zero eigenvalues of *A*.\n",
    "#### Suppose *A* is non-singular with eigenvalue $\\lambda$ and an associated eigenvector $x$. Then $\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$ with an associated eigenvector $x$, i.e.,$A^{-1}x = (\\frac{1}{\\lambda})x$  \n",
    "#### The eigenvalues of a diagonal matrix $D = diag(d_1,...d_n)$ are just the diagonal entries $d_1,...d_n$\n",
    "### 3.13 Eigenvalues and Eigenvectors of Symmetric Matrices\n",
    "#### Throughout this section, let's assume that *A* is a symmetric real matrix. We have the following properties:  \n",
    "#### 1.All eigenvalues of *A* are real numbers. We denote them by $\\lambda_1,...\\lambda_n$\n",
    "#### 2.There exists a set of eigenvectors $u_1,...u_n$ such that\n",
    "#### a) for all $i$,$u_i$ is an eigenvector with eigenvalue $\\lambda_i$ \n",
    "#### b) $u_1,...u_n$ are unit vectors and orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Matrix Calculus\n",
    "### 4.1 The Gradient\n",
    "#### Suppose that $f: R^{m*n}\\rightarrow R$ is a function that takes as input a matrix *A* of size $m*n$ and returns a real value.\n",
    "#### Then the *gradient* of $f$ (with respect to $A \\in R^{m*n}$) is the matrix of partial derivatives, defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(A) \\in R^{m*n} =\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "{\\frac{\\delta f(A)}{\\delta A_{11}}}&{\\frac{\\delta f(A)}{\\delta A_{12}}}&...&{\\frac{\\delta f(A)}{\\delta A_{1n}}}\\\\\n",
    "{\\frac{\\delta f(A)}{\\delta A_{21}}}&{\\frac{\\delta f(A)}{\\delta A_{22}}}&...&{\\frac{\\delta f(A)}{\\delta A_{2n}}}\\\\\n",
    ".......\\\\\n",
    "{\\frac{\\delta f(A)}{\\delta A_{m1}}}&{\\frac{\\delta f(A)}{\\delta A_{m2}}}&...&{\\frac{\\delta f(A)}{\\delta A_{mn}}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i.e., an m*n matrix with\n",
    "$$(\\nabla_Af(A))_{ij} = \\frac{\\delta f(A)}{\\delta A_{ij}}$$\n",
    "#### Note that the size of $\\nabla_Af(A)$ is always the same as the size of A. So if, in particular, A is just a vector $x \\in R^n$\n",
    "$$\\nabla_xf(x) = \\left[\\begin{matrix}\n",
    "\\frac{\\delta f(x)}{\\delta x_1}\\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_2}\\\\\n",
    "....\\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_n}\n",
    "\\end{matrix}\\right]$$\n",
    "#### It is very important to remember that the gradient of a function is only defined if the function is real-valued, that is, if it returns a scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It follows directly from the equivalent properties of partial derivatives that:\n",
    "$$\\nabla_x(f(x)+g(x)) = \\nabla_xf(x) + \\nabla_xg(x)$$\n",
    "#### For $$t \\in R, \\nabla_x(tf(x)) = t\\nabla_xf(x)$$\n",
    "#### Let $f: R^m \\rightarrow R$ be the function defined by $f(z) = z^Tz$,such that $\\nabla_zf(z) = 2z$. But now, consider the expression\n",
    "$$\\nabla f(Ax)$$\n",
    "#### How should this expression be interped? There are at least two possibilities:\n",
    "#### 1. In the first interpretation, recall that $\\nabla_zf(z) = 2z$. Here, we interpret $\\nabla f(Ax)$ as evaluating the gradient at the point $Ax$, hence,\n",
    "$$\\nabla f(Ax) = 2(Ax) = 2Ax \\in R^m$$\n",
    "#### 2. In the second interpretation, we consinder the quantity $f(Ax)$ as a function of the input variables $x$. More formally, let $g(x) = f(Ax)$. Then in this interpretation,\n",
    "$$\\nabla f(Ax) = \\nabla_xg(x) \\in R^n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Hessian\n",
    "#### Suppoes that $f:R^n \\rightarrow R$ is a function that takes a vector in $R^n$ and returns a real number. Then the Hessian matrix with respect to x, written $\\nabla_x^2f(x)$ or simply as H is the n*n matrix of partial derivatives,\n",
    "$$\\nabla_x^2f(x) \\in R^{n*n} = \\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\delta^2f(x)}{\\delta x_1^2}&\\frac{\\delta^2f(x)}{\\delta x_1\\delta x_2}&...&\\frac{\\delta^2f(x)}{\\delta x_1\\delta x_n}\\\\\n",
    "\\frac{\\delta^2f(x)}{\\delta x_2\\delta x_1}&\\frac{\\delta^2f(x)}{\\delta x_2^2}&...&\\frac{\\delta^2f(x)}{\\delta x_2\\delta x_n}\\\\\n",
    "......\\\\\n",
    "\\frac{\\delta^2f(x)}{\\delta x_n\\delta x_1}&\\frac{\\delta^2f(x)}{\\delta x_2\\delta_n}&...&\\frac{\\delta^2f(x)}{\\delta x_n^2}\n",
    "\\end{matrix}\\right]$$\n",
    "#### In other words, $\\nabla_x^2f(x) \\in R^{n*n}$, with\n",
    "$$(\\nabla_x^2f(x))_{ij} = \\frac{\\delta^2f(x)}{\\delta x_i\\delta x_j}$$\n",
    "#### Note that the Hessian is always symmertic, since\n",
    "$$\\frac{\\delta^2f(x)}{\\delta x_i \\delta x_j} = \\frac{\\delta^2f(x)}{\\delta x_j\\delta x_i}$$\n",
    "#### Similar to the gradient, the Hessian is defined only when $f(x)$ is real-valued.\n",
    "#### It is natural to think of the gradient as the analogue of the first derivative for functions of vectors, and the Hessian as the analogue of the second derivative. This intuition is generally correct, but there a few caveats to keep in mind.\n",
    "#### First, for real-valued functions of one variable $f: R\\rightarrow R$, it is a basic definition that the second derivative is the derivative of the first derivative, i.e.,\n",
    "$$\\frac{\\delta^2f(x)}{\\delta x^2} = \\frac{\\delta}{\\delta x}\\frac{\\delta}{\\delta x}f(x)$$\n",
    "#### However, for functions of a vector, the gradient of the function is a vector, and we cannot take the gradient of a vector ---i.e.,\n",
    "$$\\nabla_x\\nabla_xf(x) = \\nabla_x\\left[\\begin{matrix}\n",
    "\\frac{\\delta f(x)}{\\delta x_1}\\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_2}\\\\\n",
    "......\\\\\n",
    "\\frac{\\delta f(x)}{\\delta x_n}\n",
    "\\end{matrix}\\right]$$\n",
    "#### and this expression is not defined. Therefore, it is not the case that the Hessian is the gradient of the gradient. However, this is almost true, in the following sense: If we look at the $i$th entry of the gradient $(\\nabla_x f(x))_i = \\frac{\\delta f(x)}{\\delta x_i}$, and take the gradient with respect to $x$ we get\n",
    "$$\\nabla_x^2f(x) = [\\nabla_x(\\nabla_xf(x))_1,\\nabla_x(\\nabla_x(f(x))_2,......\\nabla_x(\\nabla_xf(x))_n]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Gradient and Hessians of Quadratic and Linear function\n",
    "#### For $x \\in R^n$, let $f(x) = b^Tx$ for some known vector $b \\in R^n$. Then\n",
    "$$f(x) = \\sum_{i = 1}{n}b_ix_i$$\n",
    "#### so\n",
    "$$\\frac{\\delta f(x)}{\\delta x_k} = \\frac{\\delta}{\\delta x_k}\\sum_{i = 1}^{n}b_ix_i = b_k$$\n",
    "#### From this we can easily see that $\\nabla_xb^Tx = b$. This should be compared to the analogous situation in single variable calculus, where $\\frac{\\delta}{(\\delta x_k)}\\sum_{i = 1}^{n}b_ix_i = b_k$\n",
    "#### Now consider the quadratic function $f(x) = x^TAx$ for $A \\in S^n$. Remember that \n",
    "$$f(x) = \\sum_{i = 1}^{n}\\sum_{j = 1}^{n}A_{ij}x_ix_j$$\n",
    "#### To take the partial derivative, we'll consider the terms including $x_k$ and $x^2_k$ factors seperately:\n",
    "$$\\frac{\\delta f(x)}{\\delta x_k} = \\frac{\\delta}{\\delta x_k}\\sum^{n}_{i = 1}\\sum^{n}_{j = 1}A_{ij}x_{i}x_{j}\n",
    "=\\frac{\\delta}{\\delta x_k}[\\sum_{i \\neq k}\\sum_{j \\neq k}A_{ij}x_ix_j + \\sum_{i \\neq k}A_{ik}x_ix_k + \\sum_{j \\neq k}A_{kj}x_kx_j +A_{kk}x_k^2]\n",
    "=\\sum_{i \\neq k}A_{ik}x_{i} + \\sum_{j \\neq k}A_{kj}x_j + 2A_{kk}x_k\n",
    "=\\sum_{i = 1}^{n}A_{ik}x_i + \\sum_{j \\neq 1}^{n}A_{kj}x_j\n",
    "=2 \\sum_{i = 1}^{n}A_{ki}x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### where the last equality follows since A is symmertic. Note that the $k$th entry of $\\nabla_xf(x)$ is just the inner product of the $k$th row of $A$ and $x$. Therefore ,$\\nabla_xx^TAx = 2Ax$. Again, this should remind you of the analogous fact in single-variable calculus, that $\\delta/(\\delta x)ax^2 = 2ax$\n",
    "#### Finally, let's look at the Hessian of the quadratic function $f(x) = x^TAx$,(it should be obvious that the Hessian of a linear function $b^Tx$ is zero). In this case,\n",
    "$$\\frac{\\delta ^2f(x)}{\\delta x_k \\delta x_l} = \\frac{\\delta}{\\delta x_k}[\\frac{\\delta f(x)}{\\delta x_l}]\n",
    "= \\frac{\\delta}{\\delta x_k}[2\\sum^{n}_{i = 1}A_{li}x_{i}] = 2A_{lk} = 2A_{kl}$$\n",
    "#### Therefore, it should be clear that $\\nabla_x^2x^TAx = 2A$, which should be entirely expected (and again analogous to the single-variable fact that $\\delta^2/(\\delta x^2) = 2a$).\n",
    "#### To recap,\n",
    "$$\\nabla _xb^Tb = b$$\n",
    "$$\\nabla_xx^TAx = 2Ax$$ (if A symmetric)\n",
    "$$\\nabla_x^2x^TAx = 2A$$(if A symmertic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Least Squares\n",
    "#### Using the fact that $||x||_2^2 = x^Tx$, we have \n",
    "$$||Ax - b||^2_2 = (Ax - b)^T(Ax - b) = x^TA^TAx - 2b^TAx + b^Tb$$\n",
    "#### Taking the gradient with respect to $x$ we have, and using the properties we derived in the previous section\n",
    "$$\\nabla_x(x^TA^TAx - 2b^TAx + b^Tb) = \\nabla _xx^TA^TAx - \\nabla_x2b^TAx + \\nabla_xb^Tb\n",
    "= 2A^TAx - 2A^Tb$$\n",
    "#### Setting this last expression equal to zero and solving for $x$ gives the normal equations\n",
    "$$x = (A^TA)^{-1}A^Tb$$\n",
    "#### which is the same as what we derived in class.\n",
    "### 4.5 Gradients of the Determinant\n",
    "#### Recall from our discussion of determinants that \n",
    "$$|A| = \\sum_{i = 1}^{n}(-1)^{i+j}A_{ij}|A_{\\i,\\j}|$$\n",
    "#### So \n",
    "$$\\frac{\\delta}{\\delta A_{kl}}|A| = \\frac{\\delta}{\\delta A_{kl}}\\sum_{i = 1}^{n}(-1)^{i + j}A_{ij}|A_{\\i,\\j}| = (-1)^{k+l}|A_{\\k,\\l}|\n",
    "=(adj(A))_{lk}$$\n",
    "#### From this it immediately follows from the properties of the adjoint that \n",
    "$$\\nabla _A|A| = (adj(A))^T = |A|A^{-T}$$\n",
    "#### Now let's consider the function $f: S_{++}^{n} \\rightarrow R,f(A) = log|A|$. Note that we have to restrict the domain of $f$ to be the positive definite matrices,since this ensures that $|A| \\gt 0$. so that the log of $|A|$is a real number. In this case we can use the chain rule to see that \n",
    "$$\\frac{\\delta log|A|}{\\delta A_{ij}} = \\frac{\\delta log|A|}{\\delta |A|} \\frac{\\delta |A|}{\\delta |A|}\\frac{\\delta |A|}{\\delta A_{ij}}$$\n",
    "#### From this it should be obvious that \n",
    "$$\\nabla _Alog|A| = \\frac{1}{|A|}\\nabla_A|A| = A^{-1}$$\n",
    "#### where we can drop the transpose in the last expression because A is symmertic. Note the similarity to the single-valued case, where $\\delta /(\\delta x)log x = 1/x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Eigenvalues as Optimization\n",
    "#### We use matrix calculus to solve an optimization problem in a way that leads directly to eigenvalue/eigenvector analysis. Consider the following, equality constrained optimization problem:\n",
    "$max_{x \\in R_n},x^TAx$ subject to $||x||^2_2 = 1$\n",
    "#### for a symmetric matrix $A \\in S^n$. A standard way of solving optimization with euqality constraints is by forming the $Lagrangian$,an objective function that includes the equality constraints. The Lagrangian in this case can be given by\n",
    "$$L(x,\\lambda) = x^TAx - \\lambda x^Tx$$\n",
    "#### where $\\lambda$ is called the Lagrange multiplier associated with the equality constraint. It can be established that for $x^*$ to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at $x^*$ (this is not the only condition, but it is required). That is,\n",
    "$$\\nabla _xL(x,\\lambda) = \\nabla_x(x^TAx - \\lambda x^Tx) = 2A^Tx - 2\\lambda x = 0$$\n",
    "#### Notice that this is just the linear equation $Ax = \\lambda x$. This shows that the only points which can possibly maximize (or minimize) $x^TAx = 1$ are the eigenvectors of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
