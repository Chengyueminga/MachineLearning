{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Albegra Review and Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Concepts and Notation\n",
    "## 2. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vector-Vector Products\n",
    "#### Given two vectors $x, y \\in R^n$, the quantity $x^Ty$, sometimes called the *inner product* or *dot product*.  \n",
    "#### $x^Ty \\in R = [x_1,x_2,......,x_n][y_1,y_2,......y_n]^T = \\sum_{i=1}^{n}{x_iy_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **outer product**\n",
    "#### Given vectors $x\\in R^m, y \\in R^n$(not necessarily of the same size), $xy^T \\in R^{m*n}$ is called the **outer product** of the vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left[\n",
    "\\begin{matrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_n\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "        [y_1,y_2,...,y_n] = \\left[\\begin{matrix}x_1*y_1&x_1*y_2&...&x_1*y_n\\\\\n",
    "                                   x_2*y_1&x_2*y_2&...&x_2*y_n\\\\\n",
    "                                   .......\\\\\n",
    "                                   x_m*y_1&x_m*y_2&...&x_m*y_n\n",
    "                                   \\end{matrix}\n",
    "                                   \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matrix-Vector Products\n",
    "$$ y = xA = \\left[\n",
    "              \\begin{matrix}\n",
    "              x_1,x_2 ......x_n\n",
    "              \\end{matrix}\n",
    "              \\right]\n",
    "        \\left[\n",
    "        \\begin{matrix}\n",
    "        ---a^T_1---\\\\\n",
    "        ---a^T_2---\\\\\n",
    "        .......\\\\\n",
    "        ---a^T_m---\n",
    "        \\end{matrix}\n",
    "        \\right] = x_1[---a^T_1---] + x_2[---a^T_2---]+.....+x_n[---a^T_n---]\n",
    "              $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Matrix-Matrix Products\n",
    "### C = X*Y\n",
    "$$C = XY = \\left[\\begin{matrix}x_1^T*y_1&x_1^T*y_2&...&x_1^T*y_n\\\\\n",
    "                                   x_2^T*y_1&x_2^T*y_2&...&x_2^T*y_n\\\\\n",
    "                                   .......\\\\\n",
    "                                   x_m^T*y_1&x_m^T*y_2&...&x_m^T*y_n\n",
    "                                   \\end{matrix}\n",
    "                                   \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The direct advantage of these various viewpoints is that they allow you to operate on the level/unit of vectors instead of scalars.\n",
    "#### Matrix multiplication is associative: (AB)C = A(BC)\n",
    "#### Matrix multiplication is distributive: A(B + C) = A*B + A*C\n",
    "#### Matrix multiplication is, in general, *not* commutative; that is, it can be the case that $AB \\not= BA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Operations and Properties\n",
    "### 3.1 The Identity Matrix and Diagonal Matrics\n",
    "#### *identity matrix*\n",
    "$I \\in R^{n*n}$ \n",
    "$$I_{ij} = 1, i= j$$\n",
    "$$I_{ij} = 0, i \\not= j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *diagonal matrix*\n",
    "#### D = diag{$d_1,d_2,...d_n$}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Transpose\n",
    "$A^T_{ij} = A{ji}$\n",
    "#### The following properties of transposes are easily verfied:\n",
    "### $(A^T)^T = A$   \n",
    "### $(AB)^T = B^TA^T$  \n",
    "### $(A+B)^T = A^T + B^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Symmetric Matrics\n",
    "#### *symmetric* if $A = A ^T$  \n",
    "#### *anti-symmetric* if $A = - A^T$  \n",
    "#### It is easy to show that for any matrix $A \\in R ^{n*n}$, the matrix$A + A^T$ is symmetric and the matrix$A - A^T$ is anti-symmetric\n",
    "#### $A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The Trace\n",
    "### $$trA = \\sum_{i = 1}^{n}A_{ii}$$\n",
    "#### For $A \\in R{n*n}, trA = trA^T$\n",
    "#### For $A,B \\in R^{n * n}, tr(A + B) = trA + trB$\n",
    "#### For $A \\in R^{n * n}, t \\in R, tr(tA) = t*trA$\n",
    "#### For$ A, B$ such that $AB$ is square, $trAB = trBA$  \n",
    "#### For $A,B,C$ such that $ABC$ is square, $trAB = trBA$\n",
    "#### For $A,B,C$ such that $AB$ is square,$trAB = trBA$\n",
    "#### For $A,B,C$ such that $ABC$ is square, $trABC = trBCA = trCAB$, and so on for the product of more matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Norms\n",
    "#### A *norm* of a vector ||x|| is informally a measure of the \"length\" of the vector. For example, we have the commoly-used Euclidean or $\\cal l_2$ norm,\n",
    "$$||x||_2 = \\sqrt{\\sum^n_{i = 1}x^2_i}$$\n",
    "#### Note that $||x||^2_2 = x^Tx$  \n",
    "#### 1.For all $x \\in R^n,f(x) \\geq 0$ (non-negativity).\n",
    "#### 2.$f(x) = 0$if and only if $x = 0$ (definiteness).  \n",
    "#### 3.For all $x \\in R^n, t \\in R, f(tx) = |t|f(x)$ (homogeneity).\n",
    "#### 4.For all$x,y \\in R^n, f(x + y) \\leq f(x) + f(y)$ (triangle inequality).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Linear Independence and Rank\n",
    "#### *(linearly) independent* \n",
    "#### A set of vectors${x_1,x_2,......x_n} \\subset R^m$ is said to be *(linearly) independent* if no vector can be represented as a linear combination of the remaining vectors.\n",
    "#### *(linearly) dependent*  \n",
    "#### Converselt, if one vector belonging to the set can be represented as a linear combination of the remaining vectors, then the vectirs are said to be *(linearly) dependent*, That is, if  \n",
    "$$x_n = \\sum^{n - 1}_{i = 1}\\alpha_ix_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Example\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.matrix(\"1;2;3\")\n",
    "x2 = np.matrix(\"4;1;5\")\n",
    "x3 = np.matrix(\"2;-3;-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 == -2 * x1 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For$A \\in R^{m*n}, rand(A) \\leq min(m,n). $If$ rank(A) = min(m,n),$ then $A$ is said to be *full rank*\n",
    "#### For$A \\in R^{m*n}, rank(A) = rank(A^T)$\n",
    "#### For $A \\in R^{m*n}, B \\in R^{n*p}, rank(AB) \\leq min(rank(A),rank(B))$\n",
    "#### For $A,B \\in R^{m*n}, rank(A + B) \\leq rank(A) + rank(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 The Inverse of a Square Matrix\n",
    "#### The *inverse* of a square matrix $A \\in R^{n * n}$ is denoted $A^{-1}$, and is the unique matrix such that  \n",
    "$$A^{-1}A = I = AA^{-1}$$\n",
    "#### The following are properties of the inverse; all assumes that $A,B \\in R^{n*n}$ are non-singular (invertible):  \n",
    "#### $(A^{-1})^{-1} = A$\n",
    "#### $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "#### $(A^{-1})^{T} = (A^{T})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Orthogonal Matrices\n",
    "#### Two vectors $x,y \\in R^n$are *orthogonal* if $x^Ty = 0$.   \n",
    "#### A vector $x \\in R^n$ is *normalized* if $||x||_2$ = 1  \n",
    "#### A square matrix $U \\in R^{n*n}$ is *orthogonal* if all its columns are orthogonal to each other and are normalized.  ### It follows immediately from the definition of orthogonality and normality that  \n",
    "$$U^TU = I = UU^T$$  \n",
    "#### Another nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix will not change its Euclidean norm, i.e.,\n",
    "$$||Ux||_2 = ||x||_2$$\n",
    "#### for any $x \\in R^n, U \\in R^{n*n}$ orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Range and Nullspace of a Matrix\n",
    "#### The *span* of a set of vectors $\\{x_1,x_2,.....x_n\\}$ is the set of all vectors that can be expressed as a linear combination of ${x_1,x_2,......x_n}$, That is,  \n",
    "$$span({x_1,...x_n}) = \\{ v:v = \\sum^{n}_{i = 1} \\alpha_ix_i, \\alpha_i \\in R \\}$$\n",
    "#### It can be shown that if $\\{x_1,x_2,...x_n\\}$ is a set of *n* linearly independent vectors, where each $x_i \\in R^n$, then $span(\\{x_1,...x_n\\}) = R^n$  \n",
    "#### In other words, *any* vector $v \\in R^n$ can be written as a linear combination of $x_1$ through $x_n$  \n",
    "#### The *projection* of a vactor $y \\in R^m$ onto the span of $\\{x_1,x_2,...x_n\\}$, such that $v$ is as close as possible to *y*, as measured by the Euclidean norm $||v - y||_2$. We denote the projection as Proj$(y;\\{x_1,x_2,...x_n\\}$ and can define it formally as,  \n",
    "$$Proj(y;\\{x_1,...x_n\\} = argmin_{v\\in span(\\{x_1,...x_n\\})}||y - v||_2$$\n",
    "#### The *range* (sometimes also called the columnspace) of a matrix $A \\in R^{m*n}$, denoted $\\mathcal{R}(A)$, is the span of the columns of *A*. In other words,  \n",
    "$$\\mathcal{R}(A) = \\{v \\in \\mathfrak{R}^m : v = Ax, x \\in \\mathfrak{R}^n\\}$$\n",
    "#### Making a few technical assumptions (namely that A id full rank and that n < m), the projection of a vector $y \\in \\mathfrak{R}^n$ onto the range of *A* is given by,  \n",
    "$$Proj(y;A) = argmin_{v \\in \\mathcal{R}(A)||v - y||_2} = A(A^TA)^{-1}A^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The *nullspace* of a matrix $A \\in R^{m*n}$, denoted$\\mathcal{N}(A)$ is the set of all vectors that equal 0 when multiplied by *A*, i.e.,  $$\\mathcal{N}(A) = \\{x \\in R^n: Ax = 0\\}$$\n",
    "#### Note that vectors in $\\mathcal{N}(A) \\in R^{m*n}$, denoted $\\mathcal{N}(A)$ are of size *n*, so vectors in $\\mathcal{R}(A^T)$ and $\\mathcal{N}(A)$ are both in $R^n$. In fact, we can say much more. I t turns out that  \n",
    "$$\\{w: w = u + v, u \\in \\mathcal{R}(A^T), v \\in \\mathcal{N}(A)\\} = R^n and (\\mathcal{R}(A^T)\\bigcap\\mathcal{N}(A) = \\{0\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 The Determinant\n",
    "#### The *determinant* of a square matrix $A \\in R^{n*n}$, is a function det: $R^{n*n}\\rightarrow R$,is denoted $|A|$ or det$A$.\n",
    "$$S = \\{v \\in R^n: v = \\sum_{i = 1}^{n}\\alpha_ia_i (where 0 \\leq \\alpha_i \\leq 1,i =1,....n)\\}$$\n",
    "#### Several properties that follow from the three properties above include:  \n",
    "#### For $A \\in R^{n*n}, |A| = |A^T|$\n",
    "#### For $A,B \\in R^{n*n}, |AB| = |A||B|$\n",
    "#### For$A \\in R^{n*n}, |A| = 0$ if and only *A* is singular (i.e., non-invertible). (If *A* is singular then it does not have full rank, and hence its columns are linearly dependent. In this case, the set *S* corresponds to a \"flat sheet\" within the *n*-dimensional space and hence has zero volume).  \n",
    "#### For $A \\in R^{n*n}$ and *A* non-singular, $|A^{-1}| = 1/|A|$  \n",
    "#### The *classical adjoint* (often just called the adjoint) of a matrix $A \\in R^{n*n}$, is denoted adj(A), and defined as\n",
    "$$A^{-1} = \\frac{1}{|A|}adj(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Quadratic Forms and Positive Semidefinite Matrices\n",
    "#### Given a square matrix $A \\in R^{n*n}$ and a vector $x \\in R^n$, the scalar value $x^TAx$ is called a *quadratic form*.  \n",
    "$$x^TAx = \\sum_{i = 1}^{n}\\sum_{j = 1}^{n}A_{ij}x_iy_j$$\n",
    "#### Note that,  \n",
    "$$x^TAx = (x^TAx)^T = x^TA^Tx = x^T(\\frac{1}{2}A + \\frac{1}{2}A^T)x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 Eigenvalues and Eigenvectors  \n",
    "#### Given a square matrix $A \\in R^{n*n}$, we say that $\\lambda \\in C$ is an *eigenvalue* of *A* and $x \\in C^n$ is the corresponding *eigenvector* if  \n",
    "$$Ax = \\lambda x, x \\neq 0$$\n",
    "#### We can rewrite the equation above to state that $(\\lambda,x)$ is an eigenvalue-eigenvector pair of *A* if,  \n",
    "$$(\\lambda I - A)x = 0, x \\neq 0$$\n",
    "#### But $$(\\lambda I - A)x = 0$$ has a non-zero solution to $x$ if and only if $(\\lambda I - A)$ has a non-empty nullspace, which is only the case if $|(\\lambda I  - A)| = 0$\n",
    "#### The following are properties of eigenvalues and eigenvectors (in all cases assume $A \\in R^{n*n}$ has eigenvalues $\\lambda_i,.....\\lambda_n$):\n",
    "#### The trace of a *A* is equal to the sum of its eigenvalues,  \n",
    "$$trA = \\sum_{i = 1}^{n}\\lambda_i$$\n",
    "#### The determinant of *A* is equal to the product of its eigenvalues,\n",
    "$$|A| = \\prod_{i = 1}^{n}\\lambda_i$$\n",
    "#### The rank of *A* is equal to the number of non-zero eigenvalues of *A*.\n",
    "#### Suppose *A* is non-singular with eigenvalue $\\lambda$ and an associated eigenvector $x$. Then $\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$ with an associated eigenvector $x$, i.e.,$A^{-1}x = (\\frac{1}{\\lambda})x$  \n",
    "#### The eigenvalues of a diagonal matrix $D = diag(d_1,...d_n)$ are just the diagonal entries $d_1,...d_n$\n",
    "### 3.13 Eigenvalues and Eigenvectors of Symmetric Matrices\n",
    "#### Throughout this section, let's assume that *A* is a symmetric real matrix. We have the following properties:  \n",
    "#### 1.All eigenvalues of *A* are real numbers. We denote them by $\\lambda_1,...\\lambda_n$\n",
    "#### 2.There exists a set of eigenvectors $u_1,...u_n$ such that\n",
    "#### a) for all $i$,$u_i$ is an eigenvector with eigenvalue $\\lambda_i$ \n",
    "#### b) $u_1,...u_n$ are unit vectors and orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Matrix Calculus\n",
    "### 4.1 The Gradient\n",
    "#### Suppose that $f: R^{m*n}\\rightarrow R$ is a function that takes as input a matrix *A* of size $m*n$ and returns a real value.\n",
    "#### Then the *gradient* of $f$ (with respect to $A \\in R^{m*n}$) is the matrix of partial derivatives, defined as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
